{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaarthik-balakrishnan/startkit/blob/main/challenge_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9tNzYmvAhtr"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/eeg2025/startkit/blob/main/challenge_1.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdDUQwc5QhrH"
      },
      "source": [
        "# Challenge 1: Cross-Task Transfer Learning!\n",
        "\n",
        "## How can we use the knowledge from one EEG Decoding task into another?\n",
        "\n",
        "Transfer learning is a widespread technique used in deep learning. It uses knowledge learned from one source task/domain in another target task/domain. It has been studied in depth in computer vision, natural language processing, and speech, but what about EEG brain decoding?\n",
        "\n",
        "The cross-task transfer learning scenario in EEG decoding is remarkably underexplored in comparison to the developers of new models, [Aristimunha et al., (2023)](https://arxiv.org/abs/2308.02408), even though it can be much more useful for real applications, see [Wimpff et al. (2025)](https://arxiv.org/abs/2502.06828), [Wu et al. (2025)](https://arxiv.org/abs/2507.09882).\n",
        "\n",
        "Our Challenge 1 addresses a key goal in neurotechnology: decoding cognitive function from EEG using the pre-trained knowledge from another. In other words, developing models that can effectively transfer/adapt/adjust/fine-tune knowledge from passive EEG tasks to active tasks.\n",
        "\n",
        "The ability to generalize and transfer is something critical that we believe should be focused. To go beyond just comparing metrics numbers that are often not comparable, given the specificities of EEG, such as pre-processing, inter-subject variability, and many other unique components of this type of data.\n",
        "\n",
        "This means your submitted model might be trained on a subset of tasks and fine-tuned on data from another condition, evaluating its capacity to generalize with task-specific fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2YQE0irAhtv"
      },
      "source": [
        "__________\n",
        "\n",
        "Note: For simplicity purposes, we will only show how to do the decoding directly in our target task, and it is up to the teams to think about how to use the passive task to perform the pre-training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01KvvndTF2Zm"
      },
      "source": [
        "---\n",
        "## Summary table for this start kit\n",
        "\n",
        "In this tutorial, we are going to show in more detail what we want from Challenge 1:\n",
        "\n",
        "**Contents**:\n",
        "\n",
        "0. Understand the Contrast Change Detection - CCD task.\n",
        "1. Understand the [`EEGChallengeDataset`](https://eeglab.org/EEGDash/api/eegdash.html#eegdash.EEGChallengeDataset) object.\n",
        "2. Preparing the dataloaders.\n",
        "3. Building the deep learning model with [`braindecode`](https://braindecode.org/stable/models/models_table.html).\n",
        "4. Designing the training loop.\n",
        "5. Training the model.\n",
        "6. Evaluating test performance.\n",
        "7. Going further, *benchmark go brrr!*\n",
        "\n",
        "\n",
        "\n",
        "More contents will be released during the competition inside the [`eegdash`](https://eeglab.org/EEGDash/overview.html) [examples webpage](https://eeglab.org/EEGDash/generated/auto_examples/index.html).\n",
        "\n",
        "We will establish direct communication channels with you over the next two months, resolve platform issues, and discuss other details.\n",
        "\n",
        "We are going to do weekly one-hour support time to help teams that may have difficulties with any technical aspects that may arise.\n",
        "\n",
        "We will have closer communication starting next week, 08/09/2025.\n",
        "\n",
        "Before we begin, I just want to make a deal with you, ok?\n",
        "\n",
        "This is a community competition with a strong open-source foundation. When I say open-source, I mean volunteer work.\n",
        "\n",
        "So, if you see something that does not work or could be improved, first, **please be kind**, and we will fix it together on GitHub, okay?\n",
        "\n",
        "The entire decoding community will only go further when we stop solving the same problems over and over again, and it starts working together.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZKvRi4uWTjJ"
      },
      "source": [
        "| The tutorial assumes some prior knowledge of deep learning concepts and EEG concepts:|\n",
        "| --- |\n",
        "|* Standard neural network architectures, e.g., convolutional neural networks|\n",
        "|* Optimization by batch gradient descent and backpropagation|\n",
        "|* Overfitting, early stopping, regularisation |\n",
        "|* Some knowledge of pytorch and, optionally, of the pytorch Lightning framework|\n",
        "|* That you know what EEG is |\n",
        "|* That you have basic familiarity with EEG preprocessing |\n",
        "|* Like and support open-source :) |\n",
        "\n",
        "**NOTE: You will still be able to run the whole notebook at your own pace and learn about these concepts along the way**\n",
        "**NOTE: If you just want run the code and start to play, please go to the challenge version 1, clean in the folder**\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bDfN7KATdCQ"
      },
      "source": [
        "⚠️ **In case of colab, before starting, make sure you're on a GPU instance for faster training!** ⚠️\n",
        "\n",
        "> If running on Google Colab, please request a GPU runtime by clicking `Runtime/Change runtime type` in the top bar menu, then selecting 'T4 GPU' under 'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZK2rYtCSgEl",
        "outputId": "700b2dd3-1288-429f-9d86-cd80cd0adae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA-enabled GPU found. Training should be faster.\n"
          ]
        }
      ],
      "source": [
        "# Identify whether a CUDA-enabled GPU is available\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    msg ='CUDA-enabled GPU found. Training should be faster.'\n",
        "else:\n",
        "    msg = (\n",
        "        \"No GPU found. Training will be carried out on CPU, which might be \"\n",
        "        \"slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by\"\n",
        "        \" clicking\\n`Runtime/Change runtime type` in the top bar menu, then \"\n",
        "        \"selecting \\'T4 GPU\\'\\nunder \\'Hardware accelerator\\'.\"\n",
        "    )\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCGGky2hAhtz"
      },
      "source": [
        "For the challenge, we will need two significant dependencies: `braindecode` and `eegdash`. The libraries will install PyTorch, Pytorch Audio, Scikit-learn, MNE, MNE-BIDS, and many other packages necessary for the many functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SadqlJ02D3Lo",
        "outputId": "9c60aa7f-3a46-4424-d75e-6284add1b35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: braindecode in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.10.1)\n",
            "Requirement already satisfied: mne_bids>=0.16 in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.16.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from braindecode) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from braindecode) (3.14.0)\n",
            "Requirement already satisfied: skorch>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.2.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.5.2)\n",
            "Requirement already satisfied: torchinfo~=1.8 in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.8.0)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (from braindecode) (4.3.0)\n",
            "Requirement already satisfied: linear_attention_transformer in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.19.1)\n",
            "Requirement already satisfied: docstring_inheritance in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (1.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (2.9.0.post0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.4.0)\n",
            "Requirement already satisfied: axial-positional-embedding in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.3.12)\n",
            "Requirement already satisfied: linformer>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.2.3)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.2.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (3.12.15)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (2.32.4)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.20.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.10.0->braindecode) (4.4.0)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.12/dist-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode) (0.11.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch>=1.2.0->braindecode) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb->braindecode) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.0->braindecode) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->braindecode) (3.0.2)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from local-attention->linear_attention_transformer->braindecode) (0.2.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode) (2.23)\n",
            "Requirement already satisfied: eegdash in /usr/local/lib/python3.12/dist-packages (0.3.8)\n",
            "Requirement already satisfied: braindecode>=1.0 in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.2.0)\n",
            "Requirement already satisfied: mne_bids>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.17.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from eegdash) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from eegdash) (2.3.2)\n",
            "Requirement already satisfied: pybids in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.19.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.12/dist-packages (from eegdash) (4.15.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.1.1)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/dist-packages (from eegdash) (2025.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.16.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from eegdash) (4.67.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (from eegdash) (2025.9.0)\n",
            "Requirement already satisfied: h5io>=0.2.4 in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.2.5)\n",
            "Requirement already satisfied: pymatreader in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.1.0)\n",
            "Requirement already satisfied: eeglabio in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.9.0)\n",
            "Requirement already satisfied: docstring_inheritance in /usr/local/lib/python3.12/dist-packages (from eegdash) (2.2.2)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (3.14.0)\n",
            "Requirement already satisfied: skorch>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.2.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.5.2)\n",
            "Requirement already satisfied: torchinfo~=1.8 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.8.0)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (4.3.0)\n",
            "Requirement already satisfied: linear_attention_transformer in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (0.19.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->eegdash) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->eegdash) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->eegdash) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->eegdash) (2025.2)\n",
            "Requirement already satisfied: nibabel>=4.0 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (5.3.2)\n",
            "Requirement already satisfied: formulaic>=0.3 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (1.2.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.31 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (2.0.43)\n",
            "Requirement already satisfied: bids-validator>=1.14.7 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (1.14.7.post0)\n",
            "Requirement already satisfied: num2words>=0.5.10 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (0.5.14)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (8.2.1)\n",
            "Requirement already satisfied: universal_pathlib>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (0.2.6)\n",
            "Requirement already satisfied: frozendict>=2.3 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (2.4.6)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.12/dist-packages (from pymatreader->eegdash) (1.0.2)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from pymongo->eegdash) (2.8.0)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.12/dist-packages (from s3fs->eegdash) (2.24.2)\n",
            "Requirement already satisfied: fsspec==2025.9.0 in /usr/local/lib/python3.12/dist-packages (from s3fs->eegdash) (2025.9.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from s3fs->eegdash) (3.12.15)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray->eegdash) (25.0)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (0.12.0)\n",
            "Requirement already satisfied: botocore<1.40.19,>=1.40.15 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.40.18)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.0.1)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (6.6.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.20.1)\n",
            "Requirement already satisfied: bidsschematools>=0.10 in /usr/local/lib/python3.12/dist-packages (from bids-validator>=1.14.7->pybids->eegdash) (1.1.0)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.3->pybids->eegdash) (1.3.0)\n",
            "Requirement already satisfied: narwhals>=1.17 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.3->pybids->eegdash) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.3->pybids->eegdash) (4.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (0.4)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (1.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (3.2.3)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from num2words>=0.5.10->pybids->eegdash) (0.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->eegdash) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode>=1.0->eegdash) (1.6.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.31->pybids->eegdash) (3.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.4.0)\n",
            "Requirement already satisfied: axial-positional-embedding in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.3.12)\n",
            "Requirement already satisfied: linformer>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.3)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.11)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode>=1.0->eegdash) (2.32.4)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode>=1.0->eegdash) (0.13.1)\n",
            "Requirement already satisfied: acres in /usr/local/lib/python3.12/dist-packages (from bidsschematools>=0.10->bids-validator>=1.14.7->pybids->eegdash) (0.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from bidsschematools>=0.10->bids-validator>=1.14.7->pybids->eegdash) (6.0.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.40.19,>=1.40.15->aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (2.5.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (4.4.0)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.12/dist-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode>=1.0->eegdash) (0.11.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch>=1.2.0->braindecode>=1.0->eegdash) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb->braindecode>=1.0->eegdash) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->braindecode>=1.0->eegdash) (3.0.2)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from local-attention->linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode>=1.0->eegdash) (2.23)\n"
          ]
        }
      ],
      "source": [
        "#@title ▶️ Install additional required packages for colab\n",
        "!pip install braindecode\n",
        "!pip install eegdash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Vnj2BkQhrK"
      },
      "source": [
        "## 1. What are we decoding?\n",
        "\n",
        "To start to talk about what we want to analyse, the important thing is to understand some basic concepts.\n",
        "\n",
        "-----\n",
        "\n",
        "# The brain decodes the problem\n",
        "\n",
        "Broadly speaking, here *brain decoding* is the following problem: given brain time-series signals $X \\in \\mathbb{R}^{C \\times T}$ with labels $y \\in \\mathcal{Y}$, we implement a neural network $f$ that **decodes/translates** brain activity into the target label.\n",
        "\n",
        "We aim to translate recorded brain activity into its originating stimulus, behavior, or mental state, [King, J-R. et al. (2020)](https://lauragwilliams.github.io/d/m/CognitionAlgorithm.pdf).\n",
        "\n",
        "The neural network $f$ applies a series of transformation layers (e.g., `torch.nn.Conv2d`, `torch.nn.Linear`, `torch.nn.ELU`, `torch.nn.BatchNorm2d`) to the data to filter, extract features, and learn embeddings relevant to the optimization objective—in other words:\n",
        "\n",
        "$$\n",
        "f_{\\theta}: X \\to y,\n",
        "$$\n",
        "\n",
        "where $C$ (`n_chans`) is the number of channels/electrodes and $T$ (`n_times`) is the temporal window length/epoch size over the interval of interest. Here, $\\theta$ denotes the parameters learned by the neural network.\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "For the competition, the HBN-EEG (Healthy Brain Network EEG Datasets) dataset has `n_chans = 129` with the last channels as [reference channel](https://mne.tools/stable/auto_tutorials/preprocessing/55_setting_eeg_reference.html), and we define the window length as `n_times = 200`, corresponding to 2-second windows.\n",
        "\n",
        "Your model should follow this definition exactly; any specific selection of channels, filtering, or domain-adaptation technique must be performed **within the layers of the neural network model**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJRru03PAht3"
      },
      "source": [
        "If you are interested to get more neuroscience insight, we recommend these two refereces, [HBN-EEG](https://www.biorxiv.org/content/10.1101/2024.10.03.615261v2.full.pdf) and [ Langer, N et al. (2017)](https://www.nature.com/articles/sdata201740#Sec2)\n",
        "\n",
        "Your task (**label**) is to predict the response time for the subject during this windows.\n",
        "\n",
        "In the Video, we have an example of recording cognitive activity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "uMIRuxWfAht4",
        "outputId": "2ca76c2f-1aa0-4913-9e6f-9ee8a62c54d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7e4e00fa12e0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"960\"\n",
              "            height=\"540\"\n",
              "            src=\"https://www.youtube.com/embed/tOW2Vu2zHoU?start=1630\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo(\"tOW2Vu2zHoU\", start=1630, width=960, height=540)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuQg5qWcAht6"
      },
      "source": [
        "In the figure below, we have the timeline representation of the cognitive task:\n",
        "\n",
        "![https://eeg2025.github.io/assets/img/CCD_sequence.png](https://eeg2025.github.io/assets/img/image-2.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ntDaREDAht6"
      },
      "source": [
        "Now, talking about the code, we can start to use `eegdash`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RD9B9MWEXVQ0",
        "outputId": "8a5aa17d-609c-4793-b581-ff0722488ac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "from eegdash.dataset import EEGChallengeDataset\n",
        "\n",
        "dataset_ccd = EEGChallengeDataset(task=\"contrastChangeDetection\",\n",
        "                                  release=\"R5\", cache_dir=DATA_DIR,\n",
        "                                  mini=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVCKwJPeAht7"
      },
      "source": [
        "Now, we have a Pytorch Dataset object that contains the set of recordings for the task `contrastChangeDetection`.\n",
        "\n",
        "This dataset object have very rich Raw object details that can help you to understand better the data. The framework behind this is braindecode, and if you want to understand in depth what is happening, we recommend the braindecode github itself.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YEh-QTGAht7"
      },
      "source": [
        "If you want to load the whole release you need change the `mini=False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9xSFEToAht7",
        "outputId": "de5f4aef-c321-4df0-dca3-6b8f9ec3c251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading dataset_description.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.73B/s]\n",
            "Downloading participants.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.90B/s]\n",
            "Downloading sub-NDARAH793FBF_task-contrastChangeDetection_run-1_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.91B/s]\n",
            "Downloading sub-NDARAH793FBF_task-DiaryOfAWimpyKid_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 7.10B/s]\n",
            "Downloading sub-NDARAH793FBF_task-contrastChangeDetection_run-2_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.86B/s]\n",
            "Downloading sub-NDARAH793FBF_task-ThePresent_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 7.26B/s]\n",
            "Downloading sub-NDARAH793FBF_task-symbolSearch_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.62B/s]\n",
            "Downloading sub-NDARAH793FBF_task-seqLearning8target_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.93B/s]\n",
            "Downloading sub-NDARAH793FBF_task-surroundSupp_run-1_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.68B/s]\n",
            "Downloading sub-NDARAH793FBF_task-RestingState_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.42B/s]\n",
            "Downloading sub-NDARAH793FBF_task-surroundSupp_run-2_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.79B/s]\n",
            "Downloading sub-NDARAH793FBF_task-contrastChangeDetection_run-3_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 5.10B/s]\n",
            "Downloading sub-NDARAH793FBF_task-FunwithFractals_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.47B/s]\n",
            "Downloading sub-NDARAH793FBF_task-DespicableMe_events.tsv: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.16B/s]\n",
            "Downloading task-seqLearning8target_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.58B/s]\n",
            "Downloading task-RestingState_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.61B/s]\n",
            "Downloading task-surroundSupp_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.95B/s]\n",
            "Downloading task-FunwithFractals_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 5.86B/s]\n",
            "Downloading task-symbolSearch_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.71B/s]\n",
            "Downloading task-ThePresent_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.54B/s]\n",
            "Downloading task-seqLearning6target_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.59B/s]\n",
            "Downloading task-contrastChangeDetection_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.43B/s]\n",
            "Downloading task-DespicableMe_events.json: 100%|██████████| 1.00/1.00 [00:00<00:00, 6.93B/s]"
          ]
        }
      ],
      "source": [
        "# For visualization purposes, we will see just one object.\n",
        "\n",
        "raw = dataset_ccd.datasets[0].raw  # get the Raw object of the first recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csqdbhg4Aht8"
      },
      "outputs": [],
      "source": [
        "fig = raw.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cnZu-V3Aht8"
      },
      "source": [
        "As you just realized, the eeg dash dataset object will download the dataset only when necessary, and in this case, only when we want to consume the raw data. To download all data directly, we recommend downloading the versions with Amazon API, or doing something like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9SgxLb8Aht-"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "\n",
        "raws = Parallel(n_jobs=-1)(\n",
        "    delayed(lambda d: d.raw)(d) for d in dataset_ccd.datasets\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyCxppoUQhrM"
      },
      "source": [
        "## 2. Creating the windows of interest\n",
        "\n",
        "Now that we have our raw dataset, defining the windows/epoch interval is essential.\n",
        "\n",
        "For this challenge, we test many different and semantically different regions of interest, and our preliminary studies show us that the most interesting area of interest is defined below:\n",
        "\n",
        "\n",
        "*        stimulus         │        response         │        feedback\n",
        "*                         │ *********************** (stimulus + 0.5)\n",
        "\n",
        "So we epoch after the stimulus moment with a beginning shift of 500 ms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_-gE1EgAht_"
      },
      "outputs": [],
      "source": [
        "#@title ▶️ Run this first to get all the utils functions for the epoching\n",
        "from braindecode.datasets import BaseConcatDataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1_Yv_pfAhuA"
      },
      "source": [
        "So, on our raw data, we fit the events present in it, and create a window of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxWITGyLQhrM"
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import preprocess, Preprocessor, create_windows_from_events\n",
        "from eegdash.hbn.windows import (\n",
        "    annotate_trials_with_target,\n",
        "    add_aux_anchors,\n",
        "    add_extras_columns,\n",
        "    keep_only_recordings_with,\n",
        ")\n",
        "\n",
        "EPOCH_LEN_S = 2.0\n",
        "SFREQ = 100 # by definition here\n",
        "\n",
        "transformation_offline = [\n",
        "    Preprocessor(\n",
        "        annotate_trials_with_target,\n",
        "        target_field=\"rt_from_stimulus\", epoch_length=EPOCH_LEN_S,\n",
        "        require_stimulus=True, require_response=True,\n",
        "        apply_on_array=False,\n",
        "    ),\n",
        "    Preprocessor(add_aux_anchors, apply_on_array=False),\n",
        "]\n",
        "preprocess(dataset_ccd, transformation_offline, n_jobs=1)\n",
        "\n",
        "ANCHOR = \"stimulus_anchor\"\n",
        "\n",
        "SHIFT_AFTER_STIM = 0.5\n",
        "WINDOW_LEN       = 2.0\n",
        "\n",
        "# Keep only recordings that actually contain stimulus anchors\n",
        "dataset = keep_only_recordings_with(ANCHOR, dataset_ccd)\n",
        "\n",
        "# Create single-interval windows (stim-locked, long enough to include the response)\n",
        "single_windows = create_windows_from_events(\n",
        "    dataset,\n",
        "    mapping={ANCHOR: 0},\n",
        "    trial_start_offset_samples=int(SHIFT_AFTER_STIM * SFREQ),                 # +0.5 s\n",
        "    trial_stop_offset_samples=int((SHIFT_AFTER_STIM + WINDOW_LEN) * SFREQ),   # +2.5 s\n",
        "    window_size_samples=int(EPOCH_LEN_S * SFREQ),\n",
        "    window_stride_samples=SFREQ,\n",
        "    preload=True,\n",
        ")\n",
        "\n",
        "# Injecting metadata into the extra mne annotation.\n",
        "single_windows = add_extras_columns(\n",
        "    single_windows,\n",
        "    dataset,\n",
        "    desc=ANCHOR,\n",
        "    keys=(\"target\", \"rt_from_stimulus\", \"rt_from_trialstart\",\n",
        "          \"stimulus_onset\", \"response_onset\", \"correct\", \"response_type\")\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6FKNrraQhrM"
      },
      "source": [
        "Now that we have our windowed data, we can split it into the different sets that are needed for modeling. Since our challenge focuses on generalization across subjects, we recommend dividing at the subject level.\n",
        "\n",
        "(1) the training set is used to learn the parameters of our deep learning model,  \n",
        "\n",
        "(2) the validation set is used to monitor the training process and decide when to stop it, and  \n",
        "\n",
        "(3) the test set is used to provide an estimate of the generalization performance of our model.\n",
        "\n",
        "Here, we use the last 10% of windows for testing, 10% for validation and split the remaining 80% of windows into training.\n",
        "\n",
        "**Here we go into the steps that you and your team must validate to obtain better results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toaJHKWyAhuB"
      },
      "outputs": [],
      "source": [
        "# for each windows, we can extract the metainformation using:\n",
        "\n",
        "meta_information = single_windows.get_metadata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fRiTMRuAhuB"
      },
      "outputs": [],
      "source": [
        "meta_information.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97wyzqaWfkqe"
      },
      "source": [
        "## You can inspect your target label doing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U8kuNIHfkqe"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pylab import plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax = meta_information[\"target\"].plot.hist(bins=30, ax=ax, color='lightblue')\n",
        "ax.set_xlabel(\"Response Time (s)\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_title(\"Distribution of Response Times\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wip6B3jUAhuB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "valid_frac = 0.1\n",
        "test_frac = 0.1\n",
        "seed = 2025\n",
        "\n",
        "subjects = meta_information[\"subject\"].unique()\n",
        "sub_rm = [\"NDARWV769JM7\", \"NDARME789TD2\", \"NDARUA442ZVF\", \"NDARJP304NK1\",\n",
        "          \"NDARTY128YLU\", \"NDARDW550GU6\", \"NDARLD243KRE\", \"NDARUJ292JXV\", \"NDARBA381JGH\"]\n",
        "subjects = [s for s in subjects if s not in sub_rm]\n",
        "\n",
        "train_subj, valid_test_subject = train_test_split(\n",
        "    subjects, test_size=(valid_frac + test_frac), random_state=check_random_state(seed), shuffle=True\n",
        ")\n",
        "\n",
        "valid_subj, test_subj = train_test_split(\n",
        "    valid_test_subject, test_size=test_frac, random_state=check_random_state(seed + 1), shuffle=True\n",
        ")\n",
        "# sanity check\n",
        "assert (set(valid_subj) | set(test_subj) | set(train_subj)) == set(subjects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlH0HB6oAhuC"
      },
      "outputs": [],
      "source": [
        "# and finally using braindecode split function, we can do:\n",
        "subject_split = single_windows.split(\"subject\")\n",
        "\n",
        "train_set = []\n",
        "valid_set = []\n",
        "test_set = []\n",
        "\n",
        "for s in subject_split:\n",
        "    if s in train_subj:\n",
        "        train_set.append(subject_split[s])\n",
        "    elif s in valid_subj:\n",
        "        valid_set.append(subject_split[s])\n",
        "    elif s in test_subj:\n",
        "        test_set.append(subject_split[s])\n",
        "\n",
        "train_set = BaseConcatDataset(train_set)\n",
        "valid_set = BaseConcatDataset(valid_set)\n",
        "test_set = BaseConcatDataset(test_set)\n",
        "\n",
        "print(\"Number of examples in each split in the minirelease\")\n",
        "print(f\"Train:\\t{len(train_set)}\")\n",
        "print(f\"Valid:\\t{len(valid_set)}\")\n",
        "print(f\"Test:\\t{len(test_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe7ONgC6QhrM"
      },
      "source": [
        "Finally, we create pytorch `DataLoader`s, which will be used to feed the data to the model during training and evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTIL4-0NQhrN"
      },
      "outputs": [],
      "source": [
        "# Create datasets and dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "num_workers = 1 # We are using a single worker, but you can increase this for faster data loading\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3JjNTe0QhrN"
      },
      "source": [
        "## Building the deep learning model\n",
        "\n",
        "For neural network models, **to start**, we suggest using [braindecode models](https://braindecode.org/1.2/models/models_table.html) zoo. We have implemented several different models for decoding the brain timeseries.\n",
        "\n",
        "Your team's responsibility is to develop a PyTorch module that receives the three-dimensional (`batch`, `n_chans`, `n_times`) input and outputs the contrastive response time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_UiLmm3AhuE"
      },
      "outputs": [],
      "source": [
        "from braindecode.models.util import models_dict\n",
        "\n",
        "names = sorted(models_dict)\n",
        "w = max(len(n) for n in names)\n",
        "\n",
        "for i in range(0, len(names), 3):\n",
        "    row = names[i:i+3]\n",
        "    print(\"  \".join(f\"{n:<{w}}\" for n in row))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XTeMSwYEQhrN"
      },
      "outputs": [],
      "source": [
        "# for any braindecode model, you can initialize only inputing the signal related parameters\n",
        "from braindecode.models import EEGNeX\n",
        "\n",
        "model = EEGNeX(n_chans=129, # 129 channels\n",
        "                n_outputs=1, # 1 output for regression\n",
        "                n_times=200, #2 seconds\n",
        "                sfreq=100,      # sample frequency 100 Hz\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srJ6BUiWAhuF"
      },
      "outputs": [],
      "source": [
        "# the braindecode models have this convenient way to initialize with just the signal parameters\n",
        "# and also some eegmodulemixin that allow to easily torch info once the model is created\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQu1KW_WAhuG"
      },
      "source": [
        "### The rest is our classic PyTorch/torch lighting/skorch training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e6_CvztAhuG"
      },
      "outputs": [],
      "source": [
        "# Defining training parameters\n",
        "\n",
        "lr = 1E-3\n",
        "weight_decay = 1E-5\n",
        "n_epochs = 100\n",
        "early_stopping_patience = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5CEKf58AhuG"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.nn import Module\n",
        "from torch.optim.lr_scheduler import LRScheduler\n",
        "\n",
        "# Define a method for training one epoch\n",
        "def train_one_epoch(\n",
        "    dataloader: DataLoader,\n",
        "    model: Module,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler: Optional[LRScheduler],\n",
        "    epoch: int,\n",
        "    device,\n",
        "    print_batch_stats: bool = True,\n",
        "):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    sum_sq_err = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        enumerate(dataloader), total=len(dataloader), disable=not print_batch_stats\n",
        "    )\n",
        "\n",
        "    for batch_idx, batch in progress_bar:\n",
        "        # Support datasets that may return (X, y) or (X, y, ...)\n",
        "        X, y = batch[0], batch[1]\n",
        "        X, y = X.to(device).float(), y.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        preds = model(X)\n",
        "        loss = loss_fn(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Flatten to 1D for regression metrics and accumulate squared error\n",
        "        preds_flat = preds.detach().view(-1)\n",
        "        y_flat = y.detach().view(-1)\n",
        "        sum_sq_err += torch.sum((preds_flat - y_flat) ** 2).item()\n",
        "        n_samples += y_flat.numel()\n",
        "\n",
        "        if print_batch_stats:\n",
        "            running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
        "            progress_bar.set_description(\n",
        "                f\"Epoch {epoch}, Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
        "                f\"Loss: {loss.item():.6f}, RMSE: {running_rmse:.6f}\"\n",
        "            )\n",
        "\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
        "    return avg_loss, rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DOKjhK_AhuH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Module\n",
        "from tqdm import tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def valid_model(\n",
        "    dataloader: DataLoader,\n",
        "    model: Module,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    print_batch_stats: bool = True,\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    sum_sq_err = 0.0\n",
        "    n_batches = len(dataloader)\n",
        "    n_samples = 0\n",
        "\n",
        "    iterator = tqdm(\n",
        "        enumerate(dataloader),\n",
        "        total=n_batches,\n",
        "        disable=not print_batch_stats\n",
        "    )\n",
        "\n",
        "    for batch_idx, batch in iterator:\n",
        "        # Supports (X, y) or (X, y, ...)\n",
        "        X, y = batch[0], batch[1]\n",
        "        X, y = X.to(device).float(), y.to(device).float()\n",
        "        # casting X to float32\n",
        "\n",
        "        preds = model(X)\n",
        "        batch_loss = loss_fn(preds, y).item()\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        preds_flat = preds.detach().view(-1)\n",
        "        y_flat = y.detach().view(-1)\n",
        "        sum_sq_err += torch.sum((preds_flat - y_flat) ** 2).item()\n",
        "        n_samples += y_flat.numel()\n",
        "\n",
        "        if print_batch_stats:\n",
        "            running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
        "            iterator.set_description(\n",
        "                f\"Val Batch {batch_idx + 1}/{n_batches}, \"\n",
        "                f\"Loss: {batch_loss:.6f}, RMSE: {running_rmse:.6f}\"\n",
        "            )\n",
        "\n",
        "    avg_loss = total_loss / n_batches if n_batches else float(\"nan\")\n",
        "    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
        "\n",
        "    print(f\"Val RMSE: {rmse:.6f}, Val Loss: {avg_loss:.6f}\\n\")\n",
        "    return avg_loss, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5qm3bONAhuH"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs - 1)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "patience = 5\n",
        "min_delta = 1e-4\n",
        "best_rmse = float(\"inf\")\n",
        "epochs_no_improve = 0\n",
        "best_state, best_epoch = None, None\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n",
        "\n",
        "    train_loss, train_rmse = train_one_epoch(\n",
        "        train_loader, model, loss_fn, optimizer, scheduler, epoch, device\n",
        "    )\n",
        "    val_loss, val_rmse = valid_model(test_loader, model, loss_fn, device)\n",
        "\n",
        "    print(\n",
        "        f\"Train RMSE: {train_rmse:.6f}, \"\n",
        "        f\"Average Train Loss: {train_loss:.6f}, \"\n",
        "        f\"Val RMSE: {val_rmse:.6f}, \"\n",
        "        f\"Average Val Loss: {val_loss:.6f}\"\n",
        "    )\n",
        "\n",
        "    if val_rmse < best_rmse - min_delta:\n",
        "        best_rmse = val_rmse\n",
        "        best_state = copy.deepcopy(model.state_dict())\n",
        "        best_epoch = epoch\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}. Best Val RMSE: {best_rmse:.6f} (epoch {best_epoch})\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDnh0yLeAhuI"
      },
      "outputs": [],
      "source": [
        "# saving the model\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vi0yvTTfkqq"
      },
      "source": [
        "# Common questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILCt7-fqfkqq"
      },
      "source": [
        "1) Do I need to use only braindecode?\n",
        "\n",
        "No! You can use any model that gets a tensor as input, outputs a tensor, and implements the forward method.\n",
        "\n",
        "2) Can I apply extra preprocessing/dataset loader for the hidden data?\n",
        "    \n",
        "No, any transformation needs to be inside your model. but we are super open to contributions to the libraries (eggdash and braindecode) to accommodate any reasonable request.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "startkit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}